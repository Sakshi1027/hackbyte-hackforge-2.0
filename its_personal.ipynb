{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957046c6-74e7-4e43-8738-e469b565f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.41.2 peft==0.11.1 accelerate==0.30.1 bitsandbytes==0.43.1 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472f679a-bed5-46c7-b15b-3f902c1bb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "HF_USER = \"ed-donner\"\n",
    "PROJECT_NAME = \"pricer\"\n",
    "RUN_NAME = \"2024-09-13_13.04.39\"\n",
    "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_NAME}-{RUN_NAME}\"\n",
    "REVISION = \"e8d637df551603dc86cd7a1598a8f44af4d7ae36\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84172b14-cb48-429f-a12a-3252e5e47a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1126 01:57:50.168000 28832 torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\configuration_auto.py:919: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sushm\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     13\u001b[39m REVISION = \u001b[33m\"\u001b[39m\u001b[33me8d637df551603dc86cd7a1598a8f44af4d7ae36\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 1) LOAD RAW CONFIG FIRST\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 2) PATCH rope_scaling MANUALLY\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     28\u001b[39m rs = config.rope_scaling\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\configuration_auto.py:958\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    954\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    955\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    956\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    957\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    960\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m    961\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n\u001b[32m    962\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(CONFIG_MAPPING.keys(), key=\u001b[38;5;28mlen\u001b[39m, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\configuration_utils.py:768\u001b[39m, in \u001b[36mPretrainedConfig.from_dict\u001b[39m\u001b[34m(cls, config_dict, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;66;03m# We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\u001b[39;00m\n\u001b[32m    766\u001b[39m config_dict[\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mpruned_heads\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    771\u001b[39m     config.pruned_heads = {\u001b[38;5;28mint\u001b[39m(key): value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config.pruned_heads.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\llama\\configuration_llama.py:161\u001b[39m, in \u001b[36mLlamaConfig.__init__\u001b[39m\u001b[34m(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, mlp_bias, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28mself\u001b[39m.rope_theta = rope_theta\n\u001b[32m    160\u001b[39m \u001b[38;5;28mself\u001b[39m.rope_scaling = rope_scaling\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rope_scaling_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m.attention_bias = attention_bias\n\u001b[32m    163\u001b[39m \u001b[38;5;28mself\u001b[39m.attention_dropout = attention_dropout\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\llama\\configuration_llama.py:182\u001b[39m, in \u001b[36mLlamaConfig._rope_scaling_validation\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.rope_scaling, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.rope_scaling) != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    183\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \u001b[39m\u001b[33m\"\u001b[39m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.rope_scaling\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    185\u001b[39m rope_scaling_type = \u001b[38;5;28mself\u001b[39m.rope_scaling.get(\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    186\u001b[39m rope_scaling_factor = \u001b[38;5;28mself\u001b[39m.rope_scaling.get(\u001b[33m\"\u001b[39m\u001b[33mfactor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: `rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "HF_TOKEN = \"hf_xxxxxxxxxxxxxxxxx\"\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "HF_USER = \"ed-donner\"\n",
    "PROJECT_NAME = \"pricer\"\n",
    "RUN_NAME = \"2024-09-13_13.04.39\"\n",
    "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_NAME}-{RUN_NAME}\"\n",
    "REVISION = \"e8d637df551603dc86cd7a1598a8f44af4d7ae36\"\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1) DOWNLOAD RAW config.json WITHOUT parsing\n",
    "# -------------------------------------------\n",
    "url = f\"https://huggingface.co/{BASE_MODEL}/resolve/main/config.json\"\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "raw = requests.get(url, headers=headers).json()\n",
    "\n",
    "print(\"Original rope_scaling from HF:\")\n",
    "print(raw.get(\"rope_scaling\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6dc81-b647-4a6b-bea6-9a6235d02f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"How much does this cost to the nearest dollar?\"\n",
    "PREFIX = \"Price is $\"\n",
    "\n",
    "def predict_price_usd(description: str):\n",
    "    prompt = f\"{QUESTION}\\n{description}\\n{PREFIX}\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=15,\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0])\n",
    "\n",
    "    # Extract number\n",
    "    text = decoded.split(\"Price is $\")[-1].replace(\",\", \"\")\n",
    "    match = re.search(r\"[-+]?\\d*\\.?\\d+\", text)\n",
    "    return float(match.group()) if match else None\n",
    "\n",
    "\n",
    "def usd_to_inr(amount_usd: float):\n",
    "    data = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\").json()\n",
    "    rate = data[\"rates\"][\"INR\"]\n",
    "    return amount_usd * rate, rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101313b8-a565-4d1f-84c5-7505c721f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = input(\"Enter product description:\\n\")\n",
    "\n",
    "usd = predict_price_usd(desc)\n",
    "\n",
    "if usd is None:\n",
    "    print(\"Model could not extract price.\")\n",
    "else:\n",
    "    inr, rate = usd_to_inr(usd)\n",
    "    print(\"\\n--- Price Prediction ---\")\n",
    "    print(f\"USD Price: ${usd:.2f}\")\n",
    "    print(f\"USD → INR Rate: {rate:.2f}\")\n",
    "    print(f\"Price in INR: ₹{inr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb5c1c4-83a1-47fb-af91-7819e44095ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      " - products\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"products_vectorstore\")\n",
    "\n",
    "collections = client.list_collections()\n",
    "\n",
    "print(\"Available collections:\")\n",
    "for c in collections:\n",
    "    print(\" -\", c.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2ffa43-8475-4402-ba41-8f48cfc1d616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(id=b5403c4d-8e45-45c9-a225-d5db4d225791, name=products)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"products_vectorstore\")  # your folder\n",
    "\n",
    "# list collections\n",
    "print(client.list_collections())\n",
    "\n",
    "# open collection\n",
    "collection = client.get_collection(\"products\")  # change to your collection name\n",
    "\n",
    "# get full data\n",
    "data = collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5128a2-0937-4f31-ac80-6091a0f53da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
